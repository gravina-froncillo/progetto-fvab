{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENDER CLASSIFICATION USING A SPIDER WEB METHOD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progetto di Fondamenti di Visione Artificiale e Biometria\n",
    "### Anno Accademico 2019-2020\n",
    "##### Salvatore Froncillo      0522500858\n",
    "##### Pasqualino Gravina\t\t0522500864\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Repository GitHub: https://github.com/gravina-froncillo/progetto-fvab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentazione del progetto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La gender classification è una tipologia di classificazione ampiamente utilizzata, in particolare viene utilizzata per supportare diverse biometrie di riconoscimento. Tale tipo di classificazione risulta essere molto utile, ad esempio, in ambito forense l’identificazione del genere di un soggetto mediante biometria può restringere il campo di ricerca circa del 50%. \n",
    "\n",
    "Una delle biometrie più utilizzate è il volto; quando si analizza il genere dal volto si opera con una biometria fisica.\n",
    "\n",
    "La struttura ossea di individui di genere maschile o femminile risulta essere differente e bisogna, tuttavia, tener conto che spesso il volto può essere alterato con tratti tipici della popolazione del sesso opposto a quello del soggetto (make-up, barba ecc...).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/1.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una tecnica che potrebbe ovviare a problemi di «texture» che spesso si riscontrano in algoritmi di gender recognition, può essere l’utilizzo di landmark. I landmark sono infatti punti «chiave» del volto, generalmente 68, e dopo il loro rilevamento permettono di tralasciare dettagli del volto che in alcuni casi potrebbero essere fuorvianti.\n",
    "\n",
    "Per studiare la distribuzione dei landmark sui volti si può far riferimento alla codifica di un algoritmo di pose estimation.  L’algoritmo in questione utilizza un modello a ragnatela per suddividere il volto in diversi settori, all’interno dei quali ricadono i landmark.\n",
    "\n",
    "Il numero di settori della ragnatela è fissa per ogni volto, indipendentemente dalla dimensione del volto; il settore di appartenenza di ogni landmark viene stabilito mediante semplici formule relative alle coordinate cartesiane e alle circonferenze. In figura abbiamo un esempio in cui possiamo notare in verde i 68 landmark del viso e in rosso la ragnatela.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/2.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ragnatela così ottenuta viene «srotolata» divenendo un vettore di interi i cui valori rappresentano il numero di landmark presenti in quel settore. Tale rappresentazione è molto compatta: se si vuole considerare come esempio una ragnatela composta da 4 settori per quadrante e 4 anelli si avrà un array di 64 elementi. \n",
    "\n",
    "Questo algoritmo ha inoltre prodotto ottimi risultati nella stima della posa di un soggetto; lo scopo di tale progetto è di testarne l’efficacia applbicandolo ad un problema di gender classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obiettivi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al fine di ottenere la classificazione di genere si utilizza il dataset di immagini UTKFace. Tale obiettivo può essere raggiunto mediante utilizzo dei soli dati provenienti dall’algoritmo che applica la ragnatela ai volti presenti nell’immagine. Questo processo si divide in:\n",
    "\n",
    "•\testrazione del volto dall’immagine;\n",
    "\n",
    "•\tstima della posizione dei landmarks del volto;\n",
    "\n",
    "•\tcreazione dell’array che rappresenta la ragnatela mediante l’algoritmo fornito;\n",
    "\n",
    "•\tallenare un classificatore sugli array derivanti dall’algoritmo per la creazione della ragnatela;\n",
    "\n",
    "•\tstimare l’accuratezza raggiunta per la gender classification.\n",
    "\n",
    "\n",
    "Tale progetto ha coinvolto 2 candidati, pertanto è stato richiesto di:\n",
    "\n",
    "\n",
    "•\tottenere il dataset assegnato;\n",
    "\n",
    "•\testrarre dal dataset i volti dei soggetti;\n",
    "\n",
    "•\tindividuare la posizione dei landmark;\n",
    "\n",
    "•\tutilizzare più configurazioni della ragnatela per creare diversi training set, validation set e test set\n",
    "\n",
    "•\tcreare due classificatori binari per determinare il genere, testandolo con i vari set sopra descritti;\n",
    "\n",
    "•\tvalutare le accuratezze relative alle differenti combinazioni e valutarne i pro e i contro.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset utilizzato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il dataset UTKFace è un set di dati su volti in larga scala con intervallo di età tra 0 e 116 anni. Il dataset comprende oltre 20.000 immagini di volti etichettati con età, sesso ed etnia. Le immagini coprono grandi variazioni di posa, espressione facciale, illuminazione, risoluzione, ecc. \n",
    "Le etichette riferite al volto contenuto in ogni immagine sono incluse nel nome del file e seguono tutte lo stesso schema, ovvero:\n",
    "\n",
    "[età]_[sesso]_[razza]_[data&ora].jpg\n",
    "\n",
    "•\t[età] è un numero intero compreso tra 0 e 116 e indica l'età\n",
    "\n",
    "•\t[sesso] è 0 (maschio) o 1 (femmina)\n",
    "\n",
    "•\t[razza] è un numero intero compreso tra 0 e 4, che indica bianco, nero, asiatico, indiano e altri (come ispanico, latino, mediorientale)\n",
    "\n",
    "•\t[data&ora] è nel formato di yyyymmddHHMMSSFFF, che mostra la data e l'ora in cui un'immagine è stata inserita all’interno del dataset\n",
    "\n",
    "Il dataset è fornito in due versioni differenti; la prima definita In-the-wild ovvero contenete l’immagine in formato integrale che comprende anche l’eventuale presenza di altri oggetti ed una seconda versione in cui le immagini contengono solo il volto allineato e ritagliato, tale procedimento è stato effettuato con Dlib. \n",
    "\n",
    "Per perseguire lo scopo di tale progetto è stato utilizzata la seconda versione non avendo a disposizione ingenti risorse di calcolo.\n",
    "Durante la fase di pre-processing è stata analizzata la distribuzione del dataset in base alle features indicate in etichetta per valutare quali potessero essere utili agli obiettivi prefissati; nella pagina seguente sono riportati grafici di tale analisi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Codice per analisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "dataset_folder_name = 'Dataset/utkface/image'\n",
    "dataset_dict = {\n",
    "    'razza_id': {\n",
    "        0: 'bianchi',\n",
    "        1: 'neri',\n",
    "        2: 'asiatici',\n",
    "        3: 'indiani',\n",
    "        4: 'altri'\n",
    "    },\n",
    "    'sesso_id': {\n",
    "        0: 'Maschi',\n",
    "        1: 'Femmine'\n",
    "    }\n",
    "}\n",
    "\n",
    "def parse_dataset(dataset_path, ext='jpg'):\n",
    "    def parse_info_from_file(path):\n",
    "        try:\n",
    "            filename = os.path.split(path)[1]\n",
    "            filename = os.path.splitext(filename)[0]\n",
    "            eta, sesso, razza, _ = filename.split('_')\n",
    "\n",
    "            return int(eta), dataset_dict['sesso_id'][int(sesso)], dataset_dict['razza_id'][int(razza)]\n",
    "        except Exception as ex:\n",
    "            return None, None, None\n",
    "\n",
    "    files = glob.glob(os.path.join(dataset_path, \"*.%s\" % ext))\n",
    "\n",
    "    records = []\n",
    "    for file in files:\n",
    "        info = parse_info_from_file(file)\n",
    "        records.append(info)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df['file'] = files\n",
    "    df.columns = ['Età', 'Sesso', 'Razza', 'file']\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "dataset_dict['sesso_alias'] = dict((g, i) for i, g in dataset_dict['sesso_id'].items())\n",
    "dataset_dict['razza_alias'] = dict((r, i) for i, r in dataset_dict['razza_id'].items())\n",
    "\n",
    "df = parse_dataset('Dataset/utkface/image')\n",
    "df.head()\n",
    "print(df)\n",
    "\n",
    "def plot_distribution(pd_series):\n",
    "    labels = pd_series.value_counts().index.tolist()\n",
    "    counts = pd_series.value_counts().values.tolist()\n",
    "    pie_plot = go.Pie(labels=labels, values=counts, hole=.3)\n",
    "    fig = go.Figure(data=[pie_plot])\n",
    "    fig.update_layout(title_text='Distribuzione per %s' % pd_series.name)\n",
    "    fig.show()\n",
    "\n",
    "#plot_distribution(df['Sesso'])\n",
    "#plot_distribution(df['Razza'])\n",
    "\n",
    "coppie = [0, 10, 20, 30, 40, 60, 80, np.inf]\n",
    "nomi = ['<10', '10-20', '20-30', '30-40', '40-60', '60-80', '80+']\n",
    "intervalli_eta = pd.cut(df['Età'], coppie, labels=nomi)\n",
    "\n",
    "plot_distribution(intervalli_eta)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/3.png\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/4.png\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/5.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurazioni assegnate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le quattro configurazioni assegnate sono quelle con MAE (Minor Absolute Error) minore per la pose estimation, e sono le seguenti: \n",
    "\n",
    "1.\t4C_4S_var2\n",
    "2.\t4C_4S_var4\n",
    "3.\t4C_3S_inv\n",
    "4.\t5C_4S_inv\n",
    "\n",
    "Il parametro C indica il numero di anelli di cui è composta la ragnatela e il parametro S il numero di settori per ogni quadrante.\n",
    "Per quanto concerne la differenza tra la prima e la seconda configurazione, vi è una variazione del raggio dal centro della ragnatela ai cerchi. \n",
    "\n",
    "| Configurazione| Divisione raggio              | Settori    |\n",
    "|:---------------|:-------------------------------|:------------|\n",
    "|   4C_4S_var4  | 4/10*R; 7/10*R; 9/10*R; R     | 64         |\n",
    "|   4C_4S_var2  | 8/15*R; 12/15*R; 14/15*R; R   | 64         |\n",
    "|   5C_4S_inv   | R/5; 2/5*R; 3/5*R; 4/5*R; R   | 80         |\n",
    "|   4C_3S_inv   | R/4; R/2; 3/4*R; R            | 48         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo Ragnatela"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All’inizio del progetto è stato fornito un algoritmo basato sulla configurazione 4 settori 4 cerchi per estrarre un “array ragnatela” a partire da un’immagine.\n",
    "\n",
    "L'algoritmo è diviso in:\n",
    "\n",
    "•\tface detection dell’immagine;\n",
    "\n",
    "•\testrazione dei 68 landmark del volto;\n",
    "\n",
    "•\tassegnazione di ogni landmark al settore della ragnatela corrispondente.\n",
    "\n",
    "L'algoritmo assegnato si è rivelato essere una buona base di partenza per la costruzione di una soluzione responsive dell’algoritmo ragnatela, data l’assegnazione per l’utilizzo con diverse configurazioni della ragnatela, tuttavia, sono state necessarie diverse modifiche.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo Ragnatela 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurazioni Responsive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La prima modifica apportata è stata l’aggiunta della funzione scelta (), che permette di scegliere da linea di comando quale configurazione della ragnatela applicare al dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/6.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per rendere totalmente responsive l’algoritmo è stato opportuno modificare in parte anche la funzione preesistente aggiungi()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responsive Resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nell’algoritmo originale viene effettuato il resize delle immagini ad un numero fisso, pari 512 pixel.\n",
    "\n",
    "Nella versione di seguito illustrata, tale operazione è stata resa responsive, poiché con numero fisso non è stato possibile individuare i landmark su tutte le immagini. Inoltre, si è notato che abbassando la definizione a 256 pixel l’algoritmo è più veloce e non perde molta accuratezza. \n",
    "\n",
    "Per usufruire di tutte le immagini “utilizzabili” si è scelto di aumentare gradualmente la risoluzione delle immagini ogni qualvolta non era possibile, ad una data risoluzione partendo da 256 pixel, riconoscere un volto. \n",
    "\n",
    "Dopo vari test è emerso che le immagini che sono impossibili da processare ad una definizione di 2048 pixel, non sono utilizzabili, per tale motivo si è deciso di porre come limite proprio 2048 pixel. \n",
    "\n",
    "In definitiva la funzione responsive_resize() parte da un resize a 256 pixel e aumenta fino a 2048 pixel, passando per 512 e 1024 pixel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisi immagini non utilizzabili"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche applicando la funzione responsive_resize() si è riscontrata una certa percentuale di immagini scartate dall’algoritmo. Approfondendo l’analisi, sono state riscontrate impurità nel dataset, come ad esempio volti parzialmente coperti, immagini con solo occhi e oggetti non inerenti a volti. \n",
    "Un’altra tipologia di impurità riscontrata è stata un’errata etichettatura di diverse immagini, infatti, per alcuni elementi mancano etichette nel nome file.\n",
    "\n",
    "Tali impurità sono individuate durante la fase di recupero delle etichette e non vengono aggiunte al dataset. \n",
    "\n",
    "Di seguito sono riportati alcuni esempi di impurità:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/7.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvataggio Dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infine, è stata aggiunta la funzione write_list_to_file() che salva le informazioni date in output dall’algoritmo in un file formato .csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Codice preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import threading\n",
    "import multiprocessing\n",
    "import math\n",
    "from pylab import *\n",
    "import PIL.Image as im\n",
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2\n",
    "from PIL import ImageFont\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from string import Template\n",
    "import string\n",
    "\n",
    "#--La funzione scelta consente di poter scegliere con quale tipologia di ragnatela avviare il pre-processing--#\n",
    "def scelta():\n",
    "    print(\"E' possibile scegliere tra 4 configurazioni: \")\n",
    "    print(\"   1 - 4 cerchi 4 fette Variante 4\")\n",
    "    print(\"   2 - 4 cerchi 4 fette Variante 2\")\n",
    "    print(\"   3 - 4 cerchi 3 fette \")\n",
    "    print(\"   4 - 5 cerchi 4 fette\")\n",
    "    numero = int(input(\"Quale configurazione scegli? \"))\n",
    "    if numero > 4 or numero < 1:\n",
    "        print(\"Parametro errato.\")\n",
    "        exit(0)\n",
    "    return numero\n",
    "\n",
    "def distanza(x1, y1, x2, y2):\n",
    "    x12 = (x2 - x1) * (x2 - x1)\n",
    "    y12 = (y2 - y1) * (y2 - y1)\n",
    "    xy = x12 + y12\n",
    "    dist = math.sqrt(xy)\n",
    "    return dist\n",
    "\n",
    "#Alla funzione aggiungi vengono passati due ulteriori parametri cerchi e scelta_config\n",
    "#cerchi: è necessaria per specificare in quante parti deve essere diviso il raggio per formare la ragatela\n",
    "#scelta_config: è necessaria per specificare con quale configurazione bisogna creare la ragnatela\n",
    "def aggiungi(xcentro, ycentro, rax, xpunto, ypunto, distNaso, coeff, cerchi, scelta_config):\n",
    "\n",
    "    settore = np.zeros(3) #cerchio, quadrante, fetta\n",
    "    # distNaso =  distanza dal naso\n",
    "    a = 0  # a = raggioStart\n",
    "\n",
    "    conf = [\n",
    "        ['4C_4S_var4', 4 * rax / 10, 7 * rax / 10, 9 * rax / 10],\n",
    "        ['4C_4S_var2', 8 * rax / 15, 12 * rax / 15, 14 * rax / 15],\n",
    "        ['4C_3S_inv', rax/4, rax/2, 3 * rax / 4],\n",
    "        ['5C_4S_inv', rax / 5, 2 * rax / 5, 3 * rax / 5, 4 * rax / 5]]\n",
    "\n",
    "    #---------------------------- anello ----------------------------#\n",
    "    #definisce in quale anello ci troviamo analizzando la distanza dal naso\n",
    "    #non utilizza più i parametri b1,b2, etc ma li prende dalla lista conf, l'indice viene passato alla funzione quando chiamata\n",
    "    if( distNaso > a and distNaso <= conf[scelta_config][1]):\n",
    "        settore[0] = 1\n",
    "    elif(distNaso > conf[scelta_config][1] and distNaso <= conf[scelta_config][2]):\n",
    "        settore[0] = 2\n",
    "    elif(distNaso > conf[scelta_config][2] and distNaso <= conf[scelta_config][3]):\n",
    "        settore[0] = 3\n",
    "    elif (cerchi == 5):     #è stato aggiunto un if per prevedere il quinto cerchio, non previsto nell'algoritmo fornito\n",
    "        if(distNaso > conf[scelta_config][3] and distNaso <= conf[scelta_config][4]):\n",
    "            settore[0] = 4\n",
    "        else:\n",
    "            settore[0] = 5\n",
    "    else:\n",
    "        settore [0] = 4\n",
    "    #---------------------------- quadrante --------------------------#\n",
    "    #definisce in quale quadrante ci troviamo in base alle coordinate del punto\n",
    "    if (xpunto <= xcentro and y <= ycentro):\n",
    "        # il punto appartiene al quadrante in alto a sinistra\n",
    "        settore[1] = 2\n",
    "    elif (x <= xnose and y >= ynose):\n",
    "        # il punto appartiene al quadrante in basso a sinistra\n",
    "        settore[1] = 3\n",
    "    elif (x >= xnose and y <= ynose):\n",
    "        # il punto appartiene al quadrante in alto a destra\n",
    "        settore[1] = 1\n",
    "    else:\n",
    "        # il punto appartiene al quadrante in basso a destra\n",
    "        settore[1] = 4\n",
    "#------------------------- Fetta del quadrante -----------------------#\n",
    "    b = 90  / fetteQ      #grado Stop\n",
    "    i = 1                 #in quale fetta cade il punto. i = [1, fette]\n",
    "\n",
    "    radang_a = 0                    # radiante Start\n",
    "    radang_b = math.radians(b)      # radiante Stop\n",
    "    tng_a = math.tan(radang_a)\n",
    "    tng_b = math.tan(radang_b)\n",
    "\n",
    "    #fetta\n",
    "    while(settore[2] == 0 and b < 90):\n",
    "        if coeff > tng_a and coeff <= tng_b:\n",
    "            settore[2] = i\n",
    "        b = b + (90  / fetteQ)\n",
    "        radang_b = math.radians(b)  # radiante Stop\n",
    "        tng_a = tng_b\n",
    "        tng_b = math.tan(radang_b)\n",
    "        i = i+1\n",
    "\n",
    "    if xpunto == xnose:\n",
    "        settore[2] = 1\n",
    "\n",
    "    if settore[2] == 0:\n",
    "        settore[2] = fetteQ\n",
    "\n",
    "    if settore[1] == 1 or settore[1] == 3:\n",
    "        indice = int(fette * (settore[0]-1) + fetteQ * (settore[1] - 1) + abs(settore[2] - 4) - 1)\n",
    "\n",
    "    else:\n",
    "        indice = int(fette * (settore[0] - 1) + fetteQ * (settore[1] - 1) + settore[2] - 1)\n",
    "\n",
    "    try:\n",
    "        if xnose != xpunto or ynose != ypunto:           #il naso non ha settore\n",
    "            volto[indice] = int(volto[indice] + 1)       #aggiunge 1 al contatore del settore contenente il landmark\n",
    "    except:\n",
    "        print(\"Errore, non è stato possibile addizionare il landmark al settore della ragnatela.\")\n",
    "        print(\"L'indice del landmark non addizionato  è:  \" + str(indice))\n",
    "\n",
    "#la funzione reponsive_resize riceve in input l'immagine del datataset e ne individua la presenza di un volto\n",
    "#si è notato che alcuni volti a bassa risoluzione non vengono rilevati, si prova così ad effettuare l'individuazione\n",
    "#a diverse risoluzioni fino ad un massimo di 2048, dopo tale risoluzione l'immagine o non contiene un volto o il\n",
    "#volto presenta elementi di disturbo che ne impediscono l'individuazione\n",
    "def responsive_resize(image):\n",
    "    image = imutils.resize(image, width=256)    #ridimensiona l'immagine\n",
    "    rects = detector(image, 1)                  #rileva la presenza di volti all'interno dell'immagine\n",
    "    if len(rects) == 0:\n",
    "        image = imutils.resize(image, width=512)\n",
    "        rects = detector(image, 1)\n",
    "        if len(rects) == 0:\n",
    "            image = imutils.resize(image, width=1024)\n",
    "            rects = detector(image, 1)\n",
    "            if len(rects) == 0:\n",
    "                image = imutils.resize(image, width=2048)\n",
    "                rects = detector(image, 1)\n",
    "                if len(rects) == 0:\n",
    "                    print(\"Immagine non riconosciuta: \" + nomeimmagine)\n",
    "    return rects, image\n",
    "\n",
    "def stampa_volto(x, y, w, h, shape, image):\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 153, 0), 2)\n",
    "    for (x, y) in shape:\n",
    "        cv2.circle(image, (x, y), 2, (255, 0, 0), -1)\n",
    "    cv2.imshow('image', image)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "#la funzione write_list_to_file permette di salvare all'interno di un file csv il dataset trasformato,\n",
    "#ovvero da un input di sole immagini ed etichette si avrà un nuovo dataset con un array contentente la ragnatela\n",
    "#e le rispettive altre features note dal precedente dataset.\n",
    "def write_list_to_file(guest_list, filename):\n",
    "    contatore = 0\n",
    "    with open(filename, \"w\", newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter = ';')\n",
    "        for entries in guest_list:\n",
    "            csvwriter.writerow(entries) #transf_blocks-->writerow\n",
    "            contatore = contatore +1\n",
    "        print(\"Scritti correttamente \" + str(contatore) + \" elementi. Il numero di elementi scartati  e': \" + str(size_dataset - contatore))\n",
    "    csvfile.close()\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()                                 #individua il volto il un immagine\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")   #individua i 68 landmark sul volto\n",
    "\n",
    "# indice configurazione per reperire i dati del raggio nella lista configurazioni in aggiungi, tale parametro va passato ad ogni chiamata di aggiungi\n",
    "# 0 - C4_4S_var4\n",
    "# 1 - C4_4S_var2\n",
    "# 2 - C4_3S_inv\n",
    "# 3 - C5_4S_inv\n",
    "numero_scelta = int(scelta())\n",
    "if numero_scelta==1:\n",
    "    anelli = 4\n",
    "    fetteQ = 4   # fette per quadrante\n",
    "    variante = 'var4'\n",
    "elif numero_scelta == 2:\n",
    "    anelli = 4\n",
    "    fetteQ = 4  # fette per quadrante\n",
    "    variante = 'var2'\n",
    "elif numero_scelta == 3:\n",
    "    anelli = 4\n",
    "    fetteQ = 3  # fette per quadrante\n",
    "    variante = 'inv'\n",
    "else:\n",
    "    anelli = 5\n",
    "    fetteQ = 4  # fette per quadrante\n",
    "    variante = 'inv'\n",
    "\n",
    "fette = fetteQ * 4                  #calcolo fette per anello\n",
    "n_quadranti = anelli * fette        #calcolo numero di settori totali\n",
    "\n",
    "lista_immagini = os.listdir('Dataset/utkface/image')\n",
    "size_dataset = len(lista_immagini)\n",
    "print(\"Il dataset  ha una dimensione di \" + str(size_dataset) + \" elementi\")\n",
    "lista = []                  #lista in cui andranno inseriti tutti gli array che rappresentano la ragnatela del volto presente nelle immagini (una per ogni immagine)\n",
    "num_volto = 0               #conta quante immagini abbiamo processato\n",
    "\n",
    "for img in lista_immagini:\n",
    "    if img.find(\".jpg\") > 0:\n",
    "        path_immagine = \"Dataset/utkface/image/\" + str(img)\n",
    "        foto = cv2.imread(path_immagine)\n",
    "        volto = [0 for i in range(n_quadranti)]                   #array che contengono un contatore per ogni settore della ragnatela\n",
    "        nomeimmagine = str(img)\n",
    "        try:\n",
    "            eta, sesso, razza, data = nomeimmagine.split('_')     #si ricavano età sesso e razza dal nome dell'immagine\n",
    "        except ValueError:                                        #alcuni file non sono etichettati correttamente, verranno scartati\n",
    "            continue\n",
    "\n",
    "        rects, foto = responsive_resize(foto)                     #si individua il volto con la funzione detector presente all'interno di reponsive_resize()\n",
    "        gray = cv2.cvtColor(foto, cv2.COLOR_BGR2GRAY)\n",
    "        xnose, ynose = 0, 0     #coordinate naso\n",
    "        raggio = 0              #raggio della ragnatela\n",
    "        xlont, ylont = 0, 0     #coordinate del landmark più lontano\n",
    "        distanza_punto = 0      #variabile che utilizziamo per calcolare la distanza dei landmark dal naso e trovare il raggio\n",
    "        m = 0                   #coefficiente che diamo alla funzione aggiungi\n",
    "        d = 0                   #distanza tra punto e naso che diamo alla funzione aggiungi\n",
    "        #il ciclo viene utilizzato nel caso in cui in un'immagine abbiamo più di un volto, se non trova volti va all'immagine successiva\n",
    "        for (i, rect) in enumerate(rects):\n",
    "            shape = predictor(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "            (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "            #stampa_volto(x, y, w, h, shape, foto)\n",
    "            xnose = shape[33][0]        #il naso ha il landmark numero 33\n",
    "            ynose = shape[33][1]        #il naso ha il landmark numero 33\n",
    "            for (x, y) in shape:        #scegliamo il raggio guardando le coordinate più distanti\n",
    "                distanza_punto = distanza(xnose, ynose, x, y)\n",
    "                if(distanza_punto > raggio):\n",
    "                    raggio = distanza_punto\n",
    "                    xlont = x   #coordinata x del punto più lontano dal naso\n",
    "                    ylont = y   #coordinata y del punto più lontano dal naso\n",
    "\n",
    "            for(x,y) in shape:\n",
    "                settore = [0,0,0]          #settore[0] = cerchio - settore[1] = quadrante - settore[2] = fetta\n",
    "                if(y == ynose):            #calcola il coefficiente per ogni punto da utilizzare nella funzione aggiungi\n",
    "                    m = 0\n",
    "                else:\n",
    "                    m = (x - xnose)/(y-ynose)\n",
    "                m = abs(m)                 #valore assoluto di m\n",
    "                d = distanza(xnose, ynose, x,y)\n",
    "                aggiungi(xnose, ynose, raggio, x, y, d, m, anelli, numero_scelta-1)\n",
    "            #aggiunto anelli e per settare i parametri della ragnatela in modo \"responsive\"\n",
    "            #aggiungiamo all'array di settori le 3 informazioni derivate dal nome: eta, razza, sesso, nome file\n",
    "            volto.append(int(eta))\n",
    "            volto.append(int(razza))\n",
    "            volto.append(int(sesso))\n",
    "            volto.append(nomeimmagine)\n",
    "            lista.append(volto)              #salviamo l'array dei settori nella lista\n",
    "            num_volto = num_volto + 1        #contatore numero di immagini processate\n",
    "        if (num_volto % 200) == 0:           #stampa l'avanzamento del processo\n",
    "            print(\"Ho processato \" + str(num_volto) + \" elementi\")\n",
    "\n",
    "#viene salva la lista di ragnatele con le rispettive features all'interno di un file csv con il numero che indica la configurazione\n",
    "write_list_to_file(lista, 'Dataset-'+str(anelli)+'C_'+str(fetteQ)+'S_'+str(variante)+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificatori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tale capitolo si analizzerà in dettaglio il percorso di ricerche effettuate per raggiungere la soluzione migliore in termini di accuratezza.\n",
    "\n",
    "Le tipologie di modelli di apprendimento supervisionati sviluppati per la risoluzione del problema di classificazione sono:\n",
    "\n",
    "•\tSupport Vector Machine (SVM);\n",
    "\n",
    "•\trete neurale con vettore ragnatela;\n",
    "\n",
    "•\trete neurale convoluzionale;\n",
    "\n",
    "•\trete neurale ibrida immagine e ragnatela.\n",
    "\n",
    "Nei successivi paragrafi si procederà ad analizzare nel dettaglio ogni tipologia di modello ed i relativi risultati utilizzando le configurazioni assegnate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificatore SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le Support Vector Machines (SVM) sono dei modelli di apprendimento supervisionato associati ad algoritmi di apprendimento per la classificazione e la regressione. Dato un insieme di esempi per l'addestramento, ognuno dei quali etichettato con la classe di appartenenza fra le due possibili classi, un algoritmo di addestramento per le SVM costruisce un modello che assegna i nuovi esempi a una delle due classi, ottenendo quindi un classificatore lineare binario.\n",
    "\n",
    "Un modello SVM è una rappresentazione degli esempi come punti nello spazio, mappati in modo tale che gli esempi appartenenti alle due diverse categorie siano chiaramente separati da uno spazio il più possibile ampio. I nuovi esempi sono quindi mappati nello stesso spazio e la predizione della categoria alla quale appartengono viene fatta sulla base del lato nel quale ricade.\n",
    "Nel nostro progetto l’SVM è stata la prima soluzione implementata e testata. Non ha prodotto risultati ottimi, considerando il punteggio di accuratezza pari a 0.74.\n",
    "\n",
    "Di seguito i risultati ottenuti testando le 4 configurazioni:\n",
    "\n",
    "\n",
    "| Configurazione|Accuracy    |\n",
    "|:---------------|:------------|\n",
    "|   4C_4S_var4  | 0.72       |\n",
    "|   4C_4S_var2  | 0.73       |\n",
    "|   5C_4S_inv   | 0.74       |\n",
    "|   4C_3S_inv   | 0.71       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Codice SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "dataset = pd.read_csv('Dataset/utkface/csv/Dataset-5C_4S_inv.csv', sep=';')\n",
    "X = dataset.iloc[:, 0:80].values\n",
    "print(X.shape)\n",
    "y = dataset.iloc[:, 81].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0 )\n",
    "classifier = SVC()\n",
    "history = classifier.fit(X_train, y_train)\n",
    "predicted = classifier.predict(X_test)\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, classification_report(y_test, predicted)))\n",
    "disp = sklearn.metrics.plot_confusion_matrix(classifier, X_test, y_test)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "print(\"Confusion matrix:\\n%s\" % disp.confusion_matrix)\n",
    "print(\"Accuracy Score:\")\n",
    "print(accuracy_score(y_test, predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reti Neurali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le reti neurali artificiali sono modelli matematici composti da neuroni artificiali di ispirazione alle reti neurali biologiche e sono utilizzate per risolvere problemi ingegneristici di Intelligenza Artificiale.\n",
    "Volendo dare una definizione più dettagliata potremmo dire che le reti neurali sono modelli di calcolo matematico-informatici basati sul funzionamento delle reti neurali biologiche, ossia modelli costituiti da interconnessioni di informazioni; tali interconnessioni derivano da neuroni artificiali e processi di calcolo basati sul modello delle scienze cognitive chiamato “connessionismo”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rete neurale con vettore ragnatela\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il secondo modello sviluppato utilizza una rete neurale a cui viene dato in input il dataset ricevuto in output dall’algoritmo “ragnatela”, quindi, per ogni immagine abbiamo in input un array di interi con un size variabile in base alla configurazione scelta.\n",
    "\n",
    "Per massimizzare l’accuratezza dei risultati sono stati variati in diversi modi i seguenti parametri:\n",
    "\n",
    "•\tbatch size ed Epoch del Fit;\n",
    "\n",
    "percentuale di dropout ad ogni layer;\n",
    "\n",
    "•\tbatch normalization dei layer intermedi;\n",
    "\n",
    "•\tpercentuale di elementi dati in Validation e in Test;\n",
    "\n",
    "•\tdimensione e numero dei layer intermedi;\n",
    "\n",
    "•\ttipologia di optimizer per la compilazione del classificatore;\n",
    "\n",
    "•\ttipologia di loss per la compilazione del classificatore;\n",
    "\n",
    "•\ttipologia di funzione di attivazione.\n",
    "\n",
    "È stata inoltre tentata l’aggiunta delle due features riguardanti età e razza dei volti all’array ragnatela, senza notare rilevanti miglioramenti.\n",
    "\n",
    "I risultati ottenuti sono i seguenti:\n",
    "\n",
    "| Configurazione| Loss   | Accuracy   |\n",
    "|:---------------|:--------|:------------|\n",
    "|   4C_4S_var4  | 0.54   | 0.72       |\n",
    "|   4C_4S_var2  | 0.54   | 0.73       |\n",
    "|   5C_4S_inv   | 0.50   | 0.75       |\n",
    "|   4C_3S_inv   | 0.55   | 0.70       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configurazione 4C_4S_var4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import optimizers\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import load_model\n",
    "\n",
    "dataset = pd.read_csv('Dataset/utkface/csv/Dataset-4C_4S_var4.csv', sep=';')\n",
    "print(dataset)\n",
    "dataset = shuffle(dataset)\n",
    "print(dataset)\n",
    "\n",
    "x = dataset.iloc[:, 0:64].values    #carica in x la lista di array che rappresentano le ragnatele per ogni volto, in questa configurazione ha dimensione 80\n",
    "y = dataset.iloc[:, 66].values      #carica in y l'etichetta riguardante il sesso\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)    #esegue lo split del dataset in train set e test set\n",
    "\n",
    "#classifier = load_model('spider4C_4S_var4.h5')                                                  #carica il classificatore già allenato per test successivi all'allenamento\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(output_dim = 256, init = 'uniform', activation = 'relu', input_dim = 64 ))\n",
    "classifier.add(Dense(output_dim = 256, init = 'uniform', activation = 'relu'))\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy' , metrics = ['accuracy'] )\n",
    "\n",
    "history = classifier.fit(x_train, y_train, batch_size = 4096, nb_epoch = 25, validation_split=0.15)\n",
    "\n",
    "classifier.save('spider4C_4S_var4.h5')       #salva il modello al termine dell'allenamento, in modo da poterlo caricare successivamente senza dover effettuare di nuovo l'allenamento\n",
    "\n",
    "y_pred = classifier.predict(x_test)         #esegue il test con il test set creato prima dell'allenamento\n",
    "y_pred_bool = (y_pred > 0.5)\n",
    "pd.DataFrame(y_pred_bool)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_bool)  #crea la matrice di confuzione in cui verranno riportati i TP, TF, FP, FT\n",
    "print(cm)\n",
    "\n",
    "print(\"Classification report\\n%s\\n\" % (metrics.classification_report(y_test, y_pred_bool)))     #stampa i risultati ottenuti\n",
    "\n",
    "#grafico accuratezza\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "#grafico loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configurazione 4C_4S_var2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import optimizers\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import load_model\n",
    "\n",
    "dataset = pd.read_csv('Dataset/utkface/csv/Dataset-4C_4S_var2.csv', sep=';')\n",
    "print(dataset)\n",
    "dataset = shuffle(dataset)\n",
    "print(dataset)\n",
    "\n",
    "x = dataset.iloc[:, 0:64].values    #carica in x la lista di array che rappresentano le ragnatele per ogni volto, in questa configurazione ha dimensione 80\n",
    "y = dataset.iloc[:, 66].values      #carica in y l'etichetta riguardante il sesso\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)    #esegue lo split del dataset in train set e test set\n",
    "\n",
    "#classifier = load_model('spider4C_4S_var2.h5')                                                  #carica il classificatore già allenato per test successivi all'allenamento\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(output_dim = 256, init = 'uniform', activation = 'relu', input_dim = 64 ))\n",
    "classifier.add(Dense(output_dim = 256, init = 'uniform', activation = 'relu'))\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy' , metrics = ['accuracy'] )\n",
    "\n",
    "history = classifier.fit(x_train, y_train, batch_size = 4096, nb_epoch = 25, validation_split=0.15)\n",
    "\n",
    "classifier.save('spider4C_4S_var2.h5')       #salva il modello al termine dell'allenamento, in modo da poterlo caricare successivamente senza dover effettuare di nuovo l'allenamento\n",
    "\n",
    "y_pred = classifier.predict(x_test)         #esegue il test con il test set creato prima dell'allenamento\n",
    "y_pred_bool = (y_pred > 0.5)\n",
    "pd.DataFrame(y_pred_bool)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_bool)  #crea la matrice di confuzione in cui verranno riportati i TP, TF, FP, FT\n",
    "print(cm)\n",
    "\n",
    "print(\"Classification report\\n%s\\n\" % (metrics.classification_report(y_test, y_pred_bool)))     #stampa i risultati ottenuti\n",
    "\n",
    "#grafico accuratezza\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "#grafico loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configurazione 4C_3S_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import optimizers\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import load_model\n",
    "\n",
    "dataset = pd.read_csv('Dataset/utkface/csv/Dataset-4C_3S_inv.csv', sep=';')\n",
    "print(dataset)\n",
    "dataset = shuffle(dataset)\n",
    "print(dataset)\n",
    "\n",
    "x = dataset.iloc[:, 0:48].values    #carica in x la lista di array che rappresentano le ragnatele per ogni volto, in questa configurazione ha dimensione 80\n",
    "y = dataset.iloc[:, 49].values      #carica in y l'etichetta riguardante il sesso\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)    #esegue lo split del dataset in train set e test set\n",
    "\n",
    "#classifier = load_model('spider4C_3S_inv.h5')                                                  #carica il classificatore già allenato per test successivi all'allenamento\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(output_dim = 256, init = 'uniform', activation = 'relu', input_dim = 48 ))\n",
    "classifier.add(Dense(output_dim = 256, init = 'uniform', activation = 'relu'))\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy' , metrics = ['accuracy'] )\n",
    "\n",
    "history = classifier.fit(x_train, y_train, batch_size = 4096, nb_epoch = 25, validation_split=0.15)\n",
    "\n",
    "classifier.save('spider4C_3S_inv.h5')       #salva il modello al termine dell'allenamento, in modo da poterlo caricare successivamente senza dover effettuare di nuovo l'allenamento\n",
    "\n",
    "y_pred = classifier.predict(x_test)         #esegue il test con il test set creato prima dell'allenamento\n",
    "y_pred_bool = (y_pred > 0.5)\n",
    "pd.DataFrame(y_pred_bool)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_bool)  #crea la matrice di confuzione in cui verranno riportati i TP, TF, FP, FT\n",
    "print(cm)\n",
    "\n",
    "print(\"Classification report\\n%s\\n\" % (metrics.classification_report(y_test, y_pred_bool)))     #stampa i risultati ottenuti\n",
    "\n",
    "#grafico accuratezza\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "#grafico loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configurazione 5C_4S_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import optimizers\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import load_model\n",
    "\n",
    "dataset = pd.read_csv('Dataset/utkface/csv/Dataset-5C_4S_inv.csv', sep=';')\n",
    "print(dataset)\n",
    "dataset = shuffle(dataset)\n",
    "print(dataset)\n",
    "\n",
    "x = dataset.iloc[:, 0:80].values    #carica in x la lista di array che rappresentano le ragnatele per ogni volto, in questa configurazione ha dimensione 80\n",
    "y = dataset.iloc[:, 81].values      #carica in y l'etichetta riguardante il sesso\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)    #esegue lo split del dataset in train set e test set\n",
    "\n",
    "#classifier = load_model('spider5C_4S_inv.h5')                                                  #carica il classificatore già allenato per test successivi all'allenamento\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(output_dim = 256, init = 'uniform', activation = 'relu', input_dim = 80 ))\n",
    "classifier.add(Dense(output_dim = 256, init = 'uniform', activation = 'relu'))\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy' , metrics = ['accuracy'] )\n",
    "\n",
    "history = classifier.fit(x_train, y_train, batch_size = 4096, nb_epoch = 25, validation_split=0.15)\n",
    "\n",
    "classifier.save('spider5C_4S_inv.h5')       #salva il modello al termine dell'allenamento, in modo da poterlo caricare successivamente senza dover effettuare di nuovo l'allenamento\n",
    "\n",
    "y_pred = classifier.predict(x_test)         #esegue il test con il test set creato prima dell'allenamento\n",
    "y_pred_bool = (y_pred > 0.5)\n",
    "pd.DataFrame(y_pred_bool)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_bool)  #crea la matrice di confuzione in cui verranno riportati i TP, TF, FP, FT\n",
    "print(cm)\n",
    "\n",
    "print(\"Classification report\\n%s\\n\" % (metrics.classification_report(y_test, y_pred_bool)))     #stampa i risultati ottenuti\n",
    "\n",
    "#grafico accuratezza\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "#grafico loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rete Neurale Convoluzionale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per conoscere la differenza di accuratezza tra una rete neurale basata sul solo array ragnatela e una rete neurale che prende in input l’intera immagine è stata realizzata una rete neurale convoluzionale per la classificazione dei generi.\n",
    "\n",
    "La Convolutional Neural Network (CNN) rappresenta un’architettura di rete neurale artificiale di grande successo nelle applicazioni di visione artificiale e ampiamente utilizzata anche in applicazioni che processano media come audio e video.\n",
    "\n",
    "L’applicazione più popolare di rete neurale convoluzionale resta comunque quella di identificare, da parte di un computer, cosa un’immagine rappresenta.\n",
    "\n",
    "Per massimizzare l’accuratezza del risultato sono stati variati i seguenti parametri: \n",
    "\n",
    "•\tbatch size ed Epoch del Fit;\n",
    "\n",
    "•\tpercentuale di dropout;\n",
    "\n",
    "•\tbatch normalization dei Layer intermedi;\n",
    "\n",
    "•\tpercentuale di elementi dati in Validation ed in Test;\n",
    "\n",
    "•\tdimensione e numero dei layer intermedi;\n",
    "\n",
    "•\ttipo di funzione di attivazione.\n",
    "\n",
    "L’accuracy riscontrata utilizzando questa tipologia di rete neurale è 0.88.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from joblib import dump, load\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, concatenate\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import itertools\n",
    "\n",
    "#carica il dataset per recuperare il nome delle immagini da processare\n",
    "#si è scelto di caricare le stesse immagini risultate valide nel pre-processing\n",
    "dataset = pd.read_csv('Dataset-4C_4S_var4.csv', sep=';')\n",
    "\n",
    "z_data = []\n",
    "for index, row in dataset.iterrows():\n",
    "    nome_file = str(row[67])\n",
    "    path_immagine = \"Dataset/\" + nome_file\n",
    "    face = cv2.imread(path_immagine)\n",
    "    face = cv2.resize(face, (32, 32))\n",
    "    z_data.append(face)\n",
    "    if (index % 500) == 0:\n",
    "        print(\"Ho processato \" + str(index) + \" elementi\")\n",
    "\n",
    "z = np.squeeze(z_data)\n",
    "print(z.shape)\n",
    "\n",
    "z = z.astype('float32')\n",
    "z /= 255\n",
    "print(z.shape)\n",
    "\n",
    "\n",
    "y = dataset.iloc[:, 66].values              #alla 66 colonna è specificato il sesso di ogni volto nelle immagini\n",
    "z_train, z_test, y_train, y_test = train_test_split(z, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#model = load_model('image.h5')   #carica il classificatore già allenato per test successivi all'allenamento\n",
    "\n",
    "inputA = Input(shape=(32, 32, 3))           #rete neurale convoluzionale\n",
    "A = Conv2D(64, (3, 3))(inputA)\n",
    "A = MaxPooling2D((2, 2))(A)\n",
    "A = Conv2D(32, (3, 3))(A)\n",
    "A = MaxPooling2D((2, 2))(A)\n",
    "A = Flatten()(A)\n",
    "A = Dense(256, activation=\"relu\")(A)\n",
    "A = Dense(256, activation=\"relu\")(A)\n",
    "A = Dense(1, activation=\"sigmoid\")(A)\n",
    "model = Model(inputs=inputA, outputs=A)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy' , metrics = ['accuracy'] )\n",
    "\n",
    "history = model.fit(z_train, y_train, epochs=25, batch_size= 4096, validation_split=0.15)\n",
    "model.save('image.h5')                      #salva il modello al termine dell'allenamento, in modo da poterlo caricare successivamente senza dover effettuare di nuovo l'allenamento\n",
    "\n",
    "preds = model.predict(z_test)               #esegue il test con il test set creato prima dell'allenamento\n",
    "\n",
    "preds_bool = (preds > 0.5)\n",
    "pd.DataFrame(preds_bool)\n",
    "cm = confusion_matrix(y_test, preds_bool)   #crea la matrice di confuzione in cui verranno riportati i TP, TF, FP, FT\n",
    "print(cm)\n",
    "print(\"Classification report\\n%s\\n\" % (metrics.classification_report(y_test, preds_bool)))      #stampa i risultati ottenuti\n",
    "\n",
    "#grafico accuratezza\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "#grafico loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rete Neuarle 'Ibrida'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un passo successivo è stata l’implementazione di una rete neurale ibrida, che unisce la rete neurale convoluzionale, che prende in input le sole immagini, con la rete neurale che prende in input l’array ragnatela.\n",
    "\n",
    "Le due reti si concatenano in un'unica rete che fornirà in output un risultato unico.\n",
    "\n",
    "Il modello della rete neurale convoluzionale utilizzato è il medesimo della rete convoluzionale analizzata nella soluzione precedente, in modo da poter riscontrare miglioramenti dovuti alla sola aggiunta della rete neurale con input l’array ragnatela.\n",
    "\n",
    "Anche in questo caso per massimizzare l’accuratezza del risultato sono stati variati i seguenti parametri: \n",
    "\n",
    "•\tpatch e numero di epoche del Fit;\n",
    "\n",
    "•\tpercentuale di dropout;\n",
    "\n",
    "•\tbatch normalization dei layer intermedi;\n",
    "\n",
    "•\tpercentuale di elementi dati in Validation e in Test;\n",
    "\n",
    "•\tdimensione e numero dei layer intermedi;\n",
    "\n",
    "•\ttipo di funzione di attivazione.\n",
    "\n",
    "I risultati ottenuti sono i seguenti:\n",
    "\n",
    "| Configurazione| Loss   | Accuracy   |\n",
    "|:---------------|:--------|:------------|\n",
    "|   4C_4S_var4  | 0.25   | 0.89       |\n",
    "|   4C_4S_var2  | 0.27   | 0.88       |\n",
    "|   5C_4S_inv   | 0.27   | 0.88       |\n",
    "|   4C_3S_inv   | 0.26   | 0.88       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configurazione 4C_4S_var4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, concatenate\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dataset = pd.read_csv('Dataset/utkface/csv/Dataset-4C_4S_var4.csv', sep=';')    #carica il dataset csv contenente la ragliatela e altre features\n",
    "\n",
    "z_data = []\n",
    "for index, row in dataset.iterrows():   #carica tutti i file immagini del dataset di cui si è riusciti a calcolare la ragatela\n",
    "    nome_file = str(row[67])\n",
    "    path_immagine = \"Dataset/utkface/image/\" + nome_file\n",
    "    face = cv2.imread(path_immagine)\n",
    "    face = cv2.resize(face, (32, 32))\n",
    "    z_data.append(face)\n",
    "    if (index % 500) == 0:\n",
    "        print(\"Ho processato \" + str(index) + \" elementi\")\n",
    "\n",
    "z = np.squeeze(z_data)\n",
    "print(z.shape)\n",
    "\n",
    "z = z.astype('float32')\n",
    "z /= 255\n",
    "print(z.shape)\n",
    "\n",
    "x = dataset.iloc[:, 0:64].values        #assegna ad x la ragnatela che in questa configurazione ha dimensione 64\n",
    "y = dataset.iloc[:, 66].values          #assegna ad y il target ovvero il sesso\n",
    "\n",
    "x_train, x_test_temp, y_train, y_test_temp = train_test_split(x, y, test_size=0.2, random_state=0)      #creazione train e test set l'array ragnatela\n",
    "z_train, z_test_temp, yy_train, yy_test_temp = train_test_split(z, y, test_size=0.2, random_state=0)    #creazione train e test set per le immagini\n",
    "\n",
    "#model = load_model('image4C_4S_var4.h5') #salva il modello al termine dell'allenamento, in modo da poterlo caricare successivamente senza dover effettuare di nuovo l'allenamento\n",
    "\n",
    "#il modello si compone di due reti che si concatenano durante la loro esecuzione\n",
    "inputA = Input(shape=(32, 32, 3))   #rappresenta i dati dell'immagine\n",
    "inputB = Input(shape=(64,))         #rappresenta la ragnatela\n",
    "\n",
    "A = Conv2D(64, (3, 3))(inputA)      #il ramo A è identico alla rete neurale convoluzionale sviluppata in precedenza per le sole immagini\n",
    "A = MaxPooling2D((2, 2))(A)\n",
    "A = Conv2D(32, (3, 3))(A)\n",
    "A = MaxPooling2D((2, 2))(A)\n",
    "A = Flatten()(A)\n",
    "A = Dense(256, activation=\"relu\")(A)\n",
    "A = Dense(256, activation=\"relu\")(A)\n",
    "A = Model(inputs=inputA, outputs=A)\n",
    "A.summary()\n",
    "\n",
    "B = Dense(64, activation=\"relu\")(inputB)        #il ramo B prende in input la ragnatela per ogni immagine\n",
    "B = Model(inputs=inputB, outputs=B)\n",
    "B.summary()\n",
    "combined = concatenate([A.output, B.output])    #i due rami si conatenano in un ulteriore rete fully connected\n",
    "\n",
    "C = Dense(256, activation=\"relu\")(combined)\n",
    "C = Dense(256, activation=\"relu\")(C)\n",
    "C = Dense(1, activation=\"sigmoid\")(C)\n",
    "\n",
    "model = Model(inputs=[A.input, B.input], outputs=C)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy' , metrics = ['accuracy'] )\n",
    "\n",
    "history = model.fit([z_train, x_train], yy_train, epochs=25, batch_size= 4096, validation_split=0.15)\n",
    "model.save('image4C_4S_var4.h5')                     #salva il modello al termine dell'allenamento, in modo da poterlo caricare successivamente senza dover effettuare di nuovo l'allenamento\n",
    "\n",
    "preds = model.predict([z_test_temp, x_test_temp])   #esegue il test con i test set creati prima dell'allenamento\n",
    "\n",
    "preds_bool = (preds > 0.5)\n",
    "pd.DataFrame(preds_bool)\n",
    "cm = confusion_matrix(y_test_temp, preds_bool)      #crea la matrice di confuzione in cui verranno riportati i TP, TF, FP, FT\n",
    "print(cm)\n",
    "print(\"Classification report\\n%s\\n\" % (metrics.classification_report(y_test_temp, preds_bool)))\n",
    "\n",
    "#grafico accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#grafico loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configurazione 4C_4S_var2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, concatenate\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dataset = pd.read_csv('Dataset/utkface/csv/Dataset-4C_4S_var2.csv', sep=';')    #carica il dataset csv contenente la ragliatela e altre features\n",
    "\n",
    "z_data = []\n",
    "for index, row in dataset.iterrows():   #carica tutti i file immagini del dataset di cui si è riusciti a calcolare la ragatela\n",
    "    nome_file = str(row[67])\n",
    "    path_immagine = \"Dataset/utkface/image/\" + nome_file\n",
    "    face = cv2.imread(path_immagine)\n",
    "    face = cv2.resize(face, (32, 32))\n",
    "    z_data.append(face)\n",
    "    if (index % 500) == 0:\n",
    "        print(\"Ho processato \" + str(index) + \" elementi\")\n",
    "\n",
    "z = np.squeeze(z_data)\n",
    "print(z.shape)\n",
    "\n",
    "z = z.astype('float32')\n",
    "z /= 255\n",
    "print(z.shape)\n",
    "\n",
    "x = dataset.iloc[:, 0:64].values        #assegna ad x la ragnatela che in questa configurazione ha dimensione 64\n",
    "y = dataset.iloc[:, 66].values          #assegna ad y il target ovvero il sesso\n",
    "\n",
    "x_train, x_test_temp, y_train, y_test_temp = train_test_split(x, y, test_size=0.2, random_state=0)      #creazione train e test set l'array ragnatela\n",
    "z_train, z_test_temp, yy_train, yy_test_temp = train_test_split(z, y, test_size=0.2, random_state=0)    #creazione train e test set per le immagini\n",
    "\n",
    "#model = load_model('image4C_4S_var2.h5') #salva il modello al termine dell'allenamento, in modo da poterlo caricare successivamente senza dover effettuare di nuovo l'allenamento\n",
    "\n",
    "#il modello si compone di due reti che si concatenano durante la loro esecuzione\n",
    "inputA = Input(shape=(32, 32, 3))   #rappresenta i dati dell'immagine\n",
    "inputB = Input(shape=(64,))         #rappresenta la ragnatela\n",
    "\n",
    "A = Conv2D(64, (3, 3))(inputA)      #il ramo A è identico alla rete neurale convoluzionale sviluppata in precedenza per le sole immagini\n",
    "A = MaxPooling2D((2, 2))(A)\n",
    "A = Conv2D(32, (3, 3))(A)\n",
    "A = MaxPooling2D((2, 2))(A)\n",
    "A = Flatten()(A)\n",
    "A = Dense(256, activation=\"relu\")(A)\n",
    "A = Dense(256, activation=\"relu\")(A)\n",
    "A = Model(inputs=inputA, outputs=A)\n",
    "A.summary()\n",
    "\n",
    "B = Dense(64, activation=\"relu\")(inputB)        #il ramo B prende in input la ragnatela per ogni immagine\n",
    "B = Model(inputs=inputB, outputs=B)\n",
    "B.summary()\n",
    "combined = concatenate([A.output, B.output])    #i due rami si conatenano in un ulteriore rete fully connected\n",
    "\n",
    "C = Dense(256, activation=\"relu\")(combined)\n",
    "C = Dense(256, activation=\"relu\")(C)\n",
    "C = Dense(1, activation=\"sigmoid\")(C)\n",
    "\n",
    "model = Model(inputs=[A.input, B.input], outputs=C)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy' , metrics = ['accuracy'] )\n",
    "\n",
    "history = model.fit([z_train, x_train], yy_train, epochs=25, batch_size= 4096, validation_split=0.15)\n",
    "model.save('image4C_4S_var2.h5')                     #salva il modello al termine dell'allenamento, in modo da poterlo caricare successivamente senza dover effettuare di nuovo l'allenamento\n",
    "\n",
    "preds = model.predict([z_test_temp, x_test_temp])   #esegue il test con i test set creati prima dell'allenamento\n",
    "\n",
    "preds_bool = (preds > 0.5)\n",
    "pd.DataFrame(preds_bool)\n",
    "cm = confusion_matrix(y_test_temp, preds_bool)      #crea la matrice di confuzione in cui verranno riportati i TP, TF, FP, FT\n",
    "print(cm)\n",
    "print(\"Classification report\\n%s\\n\" % (metrics.classification_report(y_test_temp, preds_bool)))\n",
    "\n",
    "#grafico accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#grafico loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configurazione 4C_3S_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, concatenate\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dataset = pd.read_csv('Dataset/utkface/csv/Dataset-4C_3S_inv.csv', sep=';')    #carica il dataset csv contenente la ragliatela e altre features\n",
    "\n",
    "z_data = []\n",
    "for index, row in dataset.iterrows():   #carica tutti i file immagini del dataset di cui si è riusciti a calcolare la ragatela\n",
    "    nome_file = str(row[51])\n",
    "    path_immagine = \"Dataset/utkface/image/\" + nome_file\n",
    "    face = cv2.imread(path_immagine)\n",
    "    face = cv2.resize(face, (32, 32))\n",
    "    z_data.append(face)\n",
    "    if (index % 500) == 0:\n",
    "        print(\"Ho processato \" + str(index) + \" elementi\")\n",
    "\n",
    "z = np.squeeze(z_data)\n",
    "print(z.shape)\n",
    "\n",
    "z = z.astype('float32')\n",
    "z /= 255\n",
    "print(z.shape)\n",
    "\n",
    "x = dataset.iloc[:, 0:48].values        #assegna ad x la ragnatela che in questa configurazione ha dimensione 64\n",
    "y = dataset.iloc[:, 49].values          #assegna ad y il target ovvero il sesso\n",
    "\n",
    "x_train, x_test_temp, y_train, y_test_temp = train_test_split(x, y, test_size=0.2, random_state=0)      #creazione train e test set l'array ragnatela\n",
    "z_train, z_test_temp, yy_train, yy_test_temp = train_test_split(z, y, test_size=0.2, random_state=0)    #creazione train e test set per le immagini\n",
    "\n",
    "#model = load_model('image4C_3S_inv.h5') #salva il modello al termine dell'allenamento, in modo da poterlo caricare successivamente senza dover effettuare di nuovo l'allenamento\n",
    "\n",
    "#il modello si compone di due reti che si concatenano durante la loro esecuzione\n",
    "inputA = Input(shape=(32, 32, 3))   #rappresenta i dati dell'immagine\n",
    "inputB = Input(shape=(48,))         #rappresenta la ragnatela\n",
    "\n",
    "A = Conv2D(64, (3, 3))(inputA)      #il ramo A è identico alla rete neurale convoluzionale sviluppata in precedenza per le sole immagini\n",
    "A = MaxPooling2D((2, 2))(A)\n",
    "A = Conv2D(32, (3, 3))(A)\n",
    "A = MaxPooling2D((2, 2))(A)\n",
    "A = Flatten()(A)\n",
    "A = Dense(256, activation=\"relu\")(A)\n",
    "A = Dense(256, activation=\"relu\")(A)\n",
    "A = Model(inputs=inputA, outputs=A)\n",
    "A.summary()\n",
    "\n",
    "B = Dense(48, activation=\"relu\")(inputB)        #il ramo B prende in input la ragnatela per ogni immagine\n",
    "B = Model(inputs=inputB, outputs=B)\n",
    "B.summary()\n",
    "combined = concatenate([A.output, B.output])    #i due rami si conatenano in un ulteriore rete fully connected\n",
    "\n",
    "C = Dense(256, activation=\"relu\")(combined)\n",
    "C = Dense(256, activation=\"relu\")(C)\n",
    "C = Dense(1, activation=\"sigmoid\")(C)\n",
    "\n",
    "model = Model(inputs=[A.input, B.input], outputs=C)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy' , metrics = ['accuracy'] )\n",
    "\n",
    "history = model.fit([z_train, x_train], yy_train, epochs=25, batch_size= 4096, validation_split=0.15)\n",
    "model.save('image4C_3S_inv.h5')                     #salva il modello al termine dell'allenamento, in modo da poterlo caricare successivamente senza dover effettuare di nuovo l'allenamento\n",
    "\n",
    "preds = model.predict([z_test_temp, x_test_temp])   #esegue il test con i test set creati prima dell'allenamento\n",
    "\n",
    "preds_bool = (preds > 0.5)\n",
    "pd.DataFrame(preds_bool)\n",
    "cm = confusion_matrix(y_test_temp, preds_bool)      #crea la matrice di confuzione in cui verranno riportati i TP, TF, FP, FT\n",
    "print(cm)\n",
    "print(\"Classification report\\n%s\\n\" % (metrics.classification_report(y_test_temp, preds_bool)))\n",
    "\n",
    "#grafico accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#grafico loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configurazione 5C_4S_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, concatenate\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dataset = pd.read_csv('Dataset/utkface/csv/Dataset-5C_4S_inv.csv', sep=';')    #carica il dataset csv contenente la ragliatela e altre features\n",
    "\n",
    "z_data = []\n",
    "for index, row in dataset.iterrows():   #carica tutti i file immagini del dataset di cui si è riusciti a calcolare la ragatela\n",
    "    nome_file = str(row[83])\n",
    "    path_immagine = \"Dataset/utkface/image/\" + nome_file\n",
    "    face = cv2.imread(path_immagine)\n",
    "    face = cv2.resize(face, (32, 32))\n",
    "    z_data.append(face)\n",
    "    if (index % 500) == 0:\n",
    "        print(\"Ho processato \" + str(index) + \" elementi\")\n",
    "\n",
    "z = np.squeeze(z_data)\n",
    "print(z.shape)\n",
    "\n",
    "z = z.astype('float32')\n",
    "z /= 255\n",
    "print(z.shape)\n",
    "\n",
    "x = dataset.iloc[:, 0:80].values        #assegna ad x la ragnatela che in questa configurazione ha dimensione 80\n",
    "y = dataset.iloc[:, 81].values          #assegna ad y il target ovvero il sesso\n",
    "\n",
    "x_train, x_test_temp, y_train, y_test_temp = train_test_split(x, y, test_size=0.2, random_state=0)      #creazione train e test set l'array ragnatela\n",
    "z_train, z_test_temp, yy_train, yy_test_temp = train_test_split(z, y, test_size=0.2, random_state=0)    #creazione train e test set per le immagini\n",
    "\n",
    "#model = load_model('image5C_4S_inv.h5') #salva il modello al termine dell'allenamento, in modo da poterlo caricare successivamente senza dover effettuare di nuovo l'allenamento\n",
    "\n",
    "#il modello si compone di due reti che si concatenano durante la loro esecuzione\n",
    "inputA = Input(shape=(32, 32, 3))   #rappresenta i dati dell'immagine\n",
    "inputB = Input(shape=(80,))         #rappresenta la ragnatela\n",
    "\n",
    "A = Conv2D(64, (3, 3))(inputA)      #il ramo A è identico alla rete neurale convoluzionale sviluppata in precedenza per le sole immagini\n",
    "A = MaxPooling2D((2, 2))(A)\n",
    "A = Conv2D(32, (3, 3))(A)\n",
    "A = MaxPooling2D((2, 2))(A)\n",
    "A = Flatten()(A)\n",
    "A = Dense(256, activation=\"relu\")(A)\n",
    "A = Dense(256, activation=\"relu\")(A)\n",
    "A = Model(inputs=inputA, outputs=A)\n",
    "A.summary()\n",
    "\n",
    "B = Dense(80, activation=\"relu\")(inputB)        #il ramo B prende in input la ragnatela per ogni immagine\n",
    "B = Model(inputs=inputB, outputs=B)\n",
    "B.summary()\n",
    "combined = concatenate([A.output, B.output])    #i due rami si conatenano in un ulteriore rete fully connected\n",
    "\n",
    "C = Dense(256, activation=\"relu\")(combined)\n",
    "C = Dense(256, activation=\"relu\")(C)\n",
    "C = Dense(1, activation=\"sigmoid\")(C)\n",
    "\n",
    "model = Model(inputs=[A.input, B.input], outputs=C)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy' , metrics = ['accuracy'] )\n",
    "\n",
    "history = model.fit([z_train, x_train], yy_train, epochs=25, batch_size= 4096, validation_split=0.15)\n",
    "model.save('image5C_4S_inv.h5')                     #salva il modello al termine dell'allenamento, in modo da poterlo caricare successivamente senza dover effettuare di nuovo l'allenamento\n",
    "\n",
    "preds = model.predict([z_test_temp, x_test_temp])   #esegue il test con i test set creati prima dell'allenamento\n",
    "\n",
    "preds_bool = (preds > 0.5)\n",
    "pd.DataFrame(preds_bool)\n",
    "cm = confusion_matrix(y_test_temp, preds_bool)      #crea la matrice di confuzione in cui verranno riportati i TP, TF, FP, FT\n",
    "print(cm)\n",
    "print(\"Classification report\\n%s\\n\" % (metrics.classification_report(y_test_temp, preds_bool)))\n",
    "\n",
    "#grafico accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#grafico loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ulteriori evidenze (Dataset CelebA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel tentativo di ottenere una percentuale migliore in termini di accuratezza si è deciso di sottoporre in input al modello, con configurazione migliore nel test sulla rete neurale con sola ragnatela, una mole maggiore di dati per l’addestramento. Per questo motivo è stato scelto l’utilizzo del dataset CelebA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset CelebA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CelebFaces Attributes Dataset (CelebA) è un set di dati di volti con attributi su larga scala con oltre 200.000 immagini di celebrità, ognuna con 40 annotazioni di attributi. Le immagini in questo dataset coprono grandi variazioni di posa e di sfondi.\n",
    "\n",
    "Tale dataset differisce da UTKFace per la tipologia di attributi utilizzati per etichettare le immagini e dalla dimensione dello stesso dataset; in UTKFace le etichette riguardano riferimenti biologici (razza, età e sesso), mentre in CelebA abbiamo un’attenzione maggiore sugli attributi riguardanti l’aspetto conferito alla persona da oggetti o segni di riconoscimento particolari (capelli ricci, occhiali, calvizie, baffi, make-up, etc..).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Codice preprocessing Dataset CelebA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import threading\n",
    "import multiprocessing\n",
    "import math\n",
    "from pylab import *\n",
    "import PIL.Image as im\n",
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2\n",
    "from PIL import ImageFont\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from string import Template\n",
    "import string\n",
    "\n",
    "#--La funzione scelta consente di poter scegliere con quale tipologia di ragnatela avviare il pre-processing--#\n",
    "def scelta():\n",
    "    print(\"E' possibile scegliere tra 4 configurazioni: \")\n",
    "    print(\"   1 - 4 cerchi 4 fette Variante 4\")\n",
    "    print(\"   2 - 4 cerchi 4 fette Variante 2\")\n",
    "    print(\"   3 - 4 cerchi 3 fette \")\n",
    "    print(\"   4 - 5 cerchi 4 fette\")\n",
    "    numero = int(input(\"Quale configurazione scegli? \"))\n",
    "    if numero > 4 or numero < 1:\n",
    "        print(\"Parametro errato.\")\n",
    "        exit(0)\n",
    "    return numero\n",
    "\n",
    "def distanza(x1, y1, x2, y2):\n",
    "    x12 = (x2 - x1) * (x2 - x1)\n",
    "    y12 = (y2 - y1) * (y2 - y1)\n",
    "    xy = x12 + y12\n",
    "    dist = math.sqrt(xy)\n",
    "    return dist\n",
    "\n",
    "#Alla funzione aggiungi vengono passati due ulteriori parametri cerchi e scelta_config\n",
    "#cerchi: è necessaria per specificare in quante parti deve essere diviso il raggio per formare la ragatela\n",
    "#scelta_config: è necessaria per specificare con quale configurazione bisogna creare la ragnatela\n",
    "def aggiungi(xcentro, ycentro, rax, xpunto, ypunto, distNaso, coeff, cerchi, scelta_config):\n",
    "\n",
    "    settore = np.zeros(3) #cerchio, quadrante, fetta\n",
    "    # distNaso =  distanza dal naso\n",
    "    a = 0  # a = raggioStart\n",
    "\n",
    "    conf = [\n",
    "        ['4C_4S_var4', 4 * rax / 10, 7 * rax / 10, 9 * rax / 10],\n",
    "        ['4C_4S_var2', 8 * rax / 15, 12 * rax / 15, 14 * rax / 15],\n",
    "        ['4C_3S_inv', rax/4, rax/2, 3 * rax / 4],\n",
    "        ['5C_4S_inv', rax / 5, 2 * rax / 5, 3 * rax / 5, 4 * rax / 5]]\n",
    "\n",
    "    #---------------------------- anello ----------------------------#\n",
    "    #definisce in quale anello ci troviamo analizzando la distanza dal naso\n",
    "    #non utilizza più i parametri b1,b2, etc ma li prende dalla lista conf, l'indice viene passato alla funzione quando chiamata\n",
    "    if( distNaso > a and distNaso <= conf[scelta_config][1]):\n",
    "        settore[0] = 1\n",
    "    elif(distNaso > conf[scelta_config][1] and distNaso <= conf[scelta_config][2]):\n",
    "        settore[0] = 2\n",
    "    elif(distNaso > conf[scelta_config][2] and distNaso <= conf[scelta_config][3]):\n",
    "        settore[0] = 3\n",
    "    elif (cerchi == 5):     #è stato aggiunto un if per prevedere il quinto cerchio, non previsto nell'algoritmo fornito\n",
    "        if(distNaso > conf[scelta_config][3] and distNaso <= conf[scelta_config][4]):\n",
    "            settore[0] = 4\n",
    "        else:\n",
    "            settore[0] = 5\n",
    "    else:\n",
    "        settore [0] = 4\n",
    "    #---------------------------- quadrante --------------------------#\n",
    "    #definisce in quale quadrante ci troviamo in base alle coordinate del punto\n",
    "    if (xpunto <= xcentro and y <= ycentro):\n",
    "        # il punto appartiene al quadrante in alto a sinistra\n",
    "        settore[1] = 2\n",
    "    elif (x <= xnose and y >= ynose):\n",
    "        # il punto appartiene al quadrante in basso a sinistra\n",
    "        settore[1] = 3\n",
    "    elif (x >= xnose and y <= ynose):\n",
    "        # il punto appartiene al quadrante in alto a destra\n",
    "        settore[1] = 1\n",
    "    else:\n",
    "        # il punto appartiene al quadrante in basso a destra\n",
    "        settore[1] = 4\n",
    "#------------------------- Fetta del quadrante -----------------------#\n",
    "    b = 90  / fetteQ      #grado Stop\n",
    "    i = 1                 #in quale fetta cade il punto. i = [1, fette]\n",
    "\n",
    "    radang_a = 0                    # radiante Start\n",
    "    radang_b = math.radians(b)      # radiante Stop\n",
    "    tng_a = math.tan(radang_a)\n",
    "    tng_b = math.tan(radang_b)\n",
    "\n",
    "    #fetta\n",
    "    while(settore[2] == 0 and b < 90):\n",
    "        if coeff > tng_a and coeff <= tng_b:\n",
    "            settore[2] = i\n",
    "        b = b + (90  / fetteQ)\n",
    "        radang_b = math.radians(b)  # radiante Stop\n",
    "        tng_a = tng_b\n",
    "        tng_b = math.tan(radang_b)\n",
    "        i = i+1\n",
    "\n",
    "    if xpunto == xnose:\n",
    "        settore[2] = 1\n",
    "\n",
    "    if settore[2] == 0:\n",
    "        settore[2] = fetteQ\n",
    "\n",
    "    if settore[1] == 1 or settore[1] == 3:\n",
    "        indice = int(fette * (settore[0]-1) + fetteQ * (settore[1] - 1) + abs(settore[2] - 4) - 1)\n",
    "\n",
    "    else:\n",
    "        indice = int(fette * (settore[0] - 1) + fetteQ * (settore[1] - 1) + settore[2] - 1)\n",
    "\n",
    "    try:\n",
    "        if xnose != xpunto or ynose != ypunto:           #il naso non ha settore\n",
    "            volto[indice] = int(volto[indice] + 1)       #aggiunge 1 al contatore del settore contenente il landmark\n",
    "    except:\n",
    "        print(\"Errore, non è stato possibile addizionare il landmark al settore della ragnatela.\")\n",
    "        print(\"L'indice del landmark non addizionato  è:  \" + str(indice))\n",
    "\n",
    "#la funzione reponsive_resize riceve in input l'immagine del datataset e ne individua la presenza di un volto\n",
    "#si è notato che alcuni volti a bassa risoluzione non vengono rilevati, si prova così ad effettuare l'individuazione\n",
    "#a diverse risoluzioni fino ad un massimo di 2048, dopo tale risoluzione l'immagine o non contiene un volto o il\n",
    "#volto presenta elementi di disturbo che ne impediscono l'individuazione\n",
    "def responsive_resize(image):\n",
    "    image = imutils.resize(image, width=256)    #ridimensiona l'immagine\n",
    "    rects = detector(image, 1)                  #rileva la presenza di volti all'interno dell'immagine\n",
    "    if len(rects) == 0:\n",
    "        image = imutils.resize(image, width=512)\n",
    "        rects = detector(image, 1)\n",
    "        if len(rects) == 0:\n",
    "            image = imutils.resize(image, width=1024)\n",
    "            rects = detector(image, 1)\n",
    "            if len(rects) == 0:\n",
    "                image = imutils.resize(image, width=2048)\n",
    "                rects = detector(image, 1)\n",
    "                if len(rects) == 0:\n",
    "                    print(\"Immagine non riconosciuta: \" + nomeimmagine)\n",
    "    return rects, image\n",
    "\n",
    "def stampa_volto(x, y, w, h, shape, image):\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 153, 0), 2)\n",
    "    for (x, y) in shape:\n",
    "        cv2.circle(image, (x, y), 2, (255, 0, 0), -1)\n",
    "    cv2.imshow('image', image)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "#la funzione write_list_to_file permette di salvare all'interno di un file csv il dataset trasformato,\n",
    "#ovvero da un input di sole immagini ed etichette si avrà un nuovo dataset con un array contentente la ragnatela\n",
    "#e le rispettive altre features note dal precedente dataset.\n",
    "def write_list_to_file(guest_list, filename):\n",
    "    contatore = 0\n",
    "    with open(filename, \"w\", newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter = ';')\n",
    "        for entries in guest_list:\n",
    "            csvwriter.writerow(entries) #transf_blocks-->writerow\n",
    "            contatore = contatore + 1\n",
    "        print(\"Scritti correttamente \" + str(contatore) + \" elementi. Il numero di elementi scartati  e': \" + str(size_dataset - contatore))\n",
    "    csvfile.close()\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()                                 #individua il volto il un immagine\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")   #individua i 68 landmark sul volto\n",
    "\n",
    "# indice configurazione per reperire i dati del raggio nella lista configurazioni in aggiungi, tale parametro va passato ad ogni chiamata di aggiungi\n",
    "# 0 - C4_4S_var4\n",
    "# 1 - C4_4S_var2\n",
    "# 2 - C4_3S_inv\n",
    "# 3 - C5_4S_inv\n",
    "numero_scelta = int(scelta())\n",
    "if numero_scelta==1:\n",
    "    anelli = 4\n",
    "    fetteQ = 4   # fette per quadrante\n",
    "    variante = 'var4'\n",
    "elif numero_scelta == 2:\n",
    "    anelli = 4\n",
    "    fetteQ = 4  # fette per quadrante\n",
    "    variante = 'var2'\n",
    "elif numero_scelta == 3:\n",
    "    anelli = 4\n",
    "    fetteQ = 3  # fette per quadrante\n",
    "    variante = 'inv'\n",
    "else:\n",
    "    anelli = 5\n",
    "    fetteQ = 4  # fette per quadrante\n",
    "    variante = 'inv'\n",
    "\n",
    "fette = fetteQ * 4                  #calcolo fette per anello\n",
    "n_quadranti = anelli * fette        #calcolo numero di settori totali\n",
    "\n",
    "lista = []                  #lista in cui andranno inseriti tutti gli array che rappresentano la ragnatela del volto presente nelle immagini (una per ogni immagine)\n",
    "num_volto = 0               #conta quante immagini abbiamo processato\n",
    "\n",
    "dataset = pd.read_csv('Dataset/celeba/csv/celeba.csv', sep=';')     #file csv contenente nome file dell'immagine e sesso del persona, differentemente da utkface le features\n",
    "                                                                    #sono presenti all'interno di un file txt. Tale file è stato epurato di molte caratteristiche non utili alle\n",
    "                                                                    #nostra finalità è convertito in csv\n",
    "lista_immagini = []\n",
    "size_dataset = len(lista_immagini)\n",
    "print(\"Il dataset  ha una dimensione di \" + str(size_dataset) + \" elementi\")\n",
    "\n",
    "#una volta caricati i nomi ed il sesso per ogni immagine in una lista\n",
    "for index, row in dataset.iterrows():\n",
    "    immagine = []\n",
    "    immagine.append(str(row[0]))\n",
    "    immagine.append(str(row[1]))\n",
    "    lista_immagini.append(immagine)\n",
    "    if (index % 500) == 0:\n",
    "        print(\"Ho inserita in lista \" + str(index) + \" elementi\")\n",
    "\n",
    "for img in lista_immagini:\n",
    "    if img.find(\".png\") > 0:\n",
    "        path_immagine = \"Dataset/celeba/image/\" + str(img[0])\n",
    "        print(path_immagine)\n",
    "        foto = cv2.imread(path_immagine)\n",
    "        volto = [0 for i in range(n_quadranti)]                   #array che contengono un contatore per ogni settore della ragnatela\n",
    "        nomeimmagine = str(img[0])\n",
    "        #rects, foto = responsive_resize(foto)          #responsive_resize disattivato per questo dataset, l'elaborazione per l'intero dataset impiegherebbe molto tempo\n",
    "        #foto = imutils.resize(foto, width=256)         #ingrandisce l'immagine\n",
    "        rects = detector(foto,1)                        #si individua il volto con la funzione detector\n",
    "        if len(rects) == 0:\n",
    "            print(\"Immagine non riconosciuta: \" + nomeimmagine)   #se rects è uguale a 0 non è stato individuato nessun volto\n",
    "        gray = cv2.cvtColor(foto, cv2.COLOR_BGR2GRAY)\n",
    "        xnose, ynose = 0, 0     #coordinate naso\n",
    "        raggio = 0              #raggio della ragnatela\n",
    "        xlont, ylont = 0, 0     #coordinate del landmark più lontano\n",
    "        distanza_punto = 0      #variabile che utilizziamo per calcolare la distanza dei landmark dal naso e trovare il raggio\n",
    "        m = 0                   #coefficiente che diamo alla funzione aggiungi\n",
    "        d = 0                   #distanza tra punto e naso che diamo alla funzione aggiungi\n",
    "        #il ciclo viene utilizzato nel caso in cui in un'immagine abbiamo più di un volto, se non trova volti va all'immagine successiva\n",
    "        for (i, rect) in enumerate(rects):\n",
    "            shape = predictor(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "            (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "            #stampa_volto(x, y, w, h, shape, foto)\n",
    "            xnose = shape[33][0]        #il naso ha il landmark numero 33\n",
    "            ynose = shape[33][1]        #il naso ha il landmark numero 33\n",
    "            for (x, y) in shape:        #scegliamo il raggio guardando le coordinate più distanti\n",
    "                distanza_punto = distanza(xnose, ynose, x, y)\n",
    "                if(distanza_punto > raggio):\n",
    "                    raggio = distanza_punto\n",
    "                    xlont = x   #coordinata x del punto più lontano dal naso\n",
    "                    ylont = y   #coordinata y del punto più lontano dal naso\n",
    "\n",
    "            for(x,y) in shape:\n",
    "                settore = [0,0,0]          # settore[0] = cerchio # settore[1] = quadrante # settore[2] = fetta\n",
    "                if(y == ynose):            # calcola il coefficiente per ogni punto da utilizzare nella funzione aggiungi\n",
    "                    m = 0\n",
    "                else:\n",
    "                    m = (x - xnose)/(y-ynose)\n",
    "                m = abs(m)                  #valore assoluto di m\n",
    "                d = distanza(xnose, ynose, x,y)\n",
    "                aggiungi(xnose, ynose, raggio, x, y, d, m, anelli, numero_scelta-1) # aggiunto anelli e o per settare i parametri della ragnatela in modo \"responsive\" (lino)\n",
    "            #aggiungiamo all'array di settori le 3 informazioni derivate dal nome: eta, sesso e razza\n",
    "            volto.append(nomeimmagine)\n",
    "            volto.append(int(img[1]))\n",
    "            lista.append(volto)              #salviamo l'array dei settori nella lista\n",
    "            num_volto = num_volto + 1  # contatore numero di immagini processate\n",
    "            if (num_volto % 200) == 0:  # stampa l'avanzamento del processo\n",
    "                print(\"Ho processato \" + str(num_volto) + \" elementi\")\n",
    "\n",
    "write_list_to_file(lista, 'Dataset/celeba/csv/Dataset-'+str(anelli)+'C_'+str(fetteQ)+'S_'+str(variante)+'.csv') #infine salviamo i dati in un file csv #aggiunto qualche parametro per nome \"responsive\" (lino)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## “Re-allenamento” modello "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avendo prima preprocessato il dataset con l’algoritmo ragnatela per ottenere l’array per ogni immagine, e in seguito allenato nuovamente il classificatore con i nuovi dati si è giunti al seguente risultato:\n",
    "\n",
    "| Configurazione| Loss   | Accuracy   |\n",
    "|:---------------|:--------|:------------|\n",
    "|   5C_4S_inv   | 0.42   | 0.81       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \"Re-allenamento” modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import optimizers\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import load_model\n",
    "\n",
    "dataset = pd.read_csv('Dataset/celeba/csv/Dataset-5C_4S_inv.csv', sep=';')\n",
    "print(dataset)\n",
    "dataset = shuffle(dataset)\n",
    "print(dataset)\n",
    "\n",
    "x = dataset.iloc[:, 0:80].values    #carica in x la lista di array che rappresentano le ragnatele per ogni volto, in questa configurazione ha dimensione 80\n",
    "y = dataset.iloc[:, 81].values      #carica in y l'etichetta riguardante il sesso\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)          #esegue lo split del dataset in train set e test set\n",
    "\n",
    "#classifier = load_model('spiderCelebA5C_4S_inv.h5')                                                  #carica il classificatore già allenato per test successivi all'allenamento\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(output_dim = 256, init = 'uniform', activation = 'relu', input_dim = 80 ))\n",
    "classifier.add(Dense(output_dim = 256, init = 'uniform', activation = 'relu'))\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy' , metrics = ['accuracy'] )\n",
    "\n",
    "history = classifier.fit(x_train, y_train, batch_size = 8192, nb_epoch = 25, validation_split=0.15)\n",
    "\n",
    "classifier.save('spiderCelebA5C_4S_inv.h5')       #salva il modello al termine dell'allenamento, in modo da poterlo caricare successivamente senza dover effettuare di nuovo l'allenamento\n",
    "\n",
    "y_pred = classifier.predict(x_test)         #esegue il test con il test set creato prima dell'allenamento\n",
    "y_pred_bool = (y_pred > 0.5)\n",
    "pd.DataFrame(y_pred_bool)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_bool)  #crea la matrice di confuzione in cui verranno riportati i TP, TF, FP, FT\n",
    "print(cm)\n",
    "\n",
    "print(\"Classification report\\n%s\\n\" % (metrics.classification_report(y_test, y_pred_bool)))     #stampa i risultati ottenuti\n",
    "\n",
    "#grafico accuratezza\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "#grafico loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Incrociati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infine, si è provveduto a testare il classificatore allenato sul dataset UTKFace sul dataset CelebA e viceversa. Le evidenze che emergono da tali test sono le seguenti:\n",
    "\n",
    "| Classificatore allenato su| Dataset di test   | Accuracy   |\n",
    "|:---------------|:--------|:------------|\n",
    "|UTKFace|\tUTKFace|\t0.74|\n",
    "|CelebA|\tCelebA|\t0.81|\n",
    "|UTKFace|\tCelebA|\t0.74|\n",
    "|CelebA|\tUTKFace|\t0.73|\n",
    "\n",
    "Dalla tabella è emerso che il classificatore addestrato con UTKFace raggiunge un punteggio di accuratezza che resta invariato su entrambi i dataset di test.\n",
    "\n",
    "Invece, nonostante il classificatore addestrato con CelebA abbia raggiunto un’accuratezza di 0.81 in fase di test con il proprio dataset, con il test incrociato tale valore diminuisce. Tale diminuzione è imputabile alla differenza di composizione dei due dataset; in UTKFace sono presenti volti in una fascia di età che spazia tra gli 0 e i 116 anni, con una distribuzione per razza diversificata. In CelebA, invece, abbiamo un range di età più ristretto, in cui non sono inclusi volti in età infantile ed avanzata. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ricerca della soluzione migliore è iniziata con l’utilizzo di classificatori di tipo SVM; tale tipologia ha dimostrato, sulle 4 configurazioni utilizzate, un’accuratezza massima di 0.74.\n",
    "\n",
    "Ritenendo non ottimale il risultato raggiunto, si è proseguito con l’implementazione di un classificatore basato su una rete neurale che prende in input il dataset dato in output dall’ algoritmo “ragnatela”. Anche in questo caso, testando l’algoritmo sulle 4 configurazioni, non si è superato il limite di accuratezza di 0.75.\n",
    "\n",
    "Per conoscere la differenza di accuratezza tra una rete neurale basata sul solo array ragnatela e una rete neurale che prende in input l’intera immagine è stata realizzata una rete neurale convoluzionale per la classificazione dei generi che ha portato ad un’accuratezza di 0.88.\n",
    "\n",
    "Successivamente è stata l’implementata una rete neurale ibrida, che unisce la rete neurale convoluzionale con la rete neurale che prende in input l’array ragnatela; tale modello produce un’accuratezza pari a 0.89. Seppur piccola, la variazione di accuratezza tra i due ultimi modelli dimostra un leggero contributo derivante dall’utilizzo della ragnatela.\n",
    "\n",
    "Nel tentativo di ottenere una percentuale migliore in termini di accuratezza nel modello della rete neurale con solo vettore ragnatela, si è deciso di sottoporre in input a tale modello una mole maggiore di dati con l’utilizzo del dataset CelebA.\n",
    "\n",
    "Questo tentativo ha prodotto un'accuratezza di 0.81, dato che risulta, però, inficiato dalla presenza all’interno di CelebA di volti raffiguranti solo persone in un range di età più stretto rispetto ad UTKFace.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
